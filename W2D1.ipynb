{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\bar CÃÑ\n",
    "# \\dot ƒä\n",
    "# \\ddot CÃà\n",
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)(60000,)\n"
     ]
    }
   ],
   "source": [
    "struct MNIST\n",
    "    train_datas::AbstractArray\n",
    "    train_labels::AbstractArray\n",
    "    test_datas::AbstractArray\n",
    "    test_labels::AbstractArray\n",
    "end\n",
    "\n",
    "function load_mnist(filepath=\"/Users/yanghanlin/Desktop/julia-2-week/dataset/\")\n",
    "    train_label_file = \"train-labels-idx1-ubyte\"\n",
    "    train_data_file = \"train-images-idx3-ubyte\"\n",
    "    \n",
    "    test_label_file = \"t10k-labels-idx1-ubyte\"\n",
    "    test_data_file = \"t10k-images-idx3-ubyte\"\n",
    "    \n",
    "    train_labels = Array{UInt8, 1}()\n",
    "    train_datas = []\n",
    "    \n",
    "    test_labels = Array{UInt8, 1}()\n",
    "    test_datas = []\n",
    "    \n",
    "    devnull = Array{UInt8, 1}()\n",
    "    open(filepath*train_label_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 8)\n",
    "        readbytes!(f, train_labels, 60000)\n",
    "    end\n",
    "    open(filepath*train_data_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 16)\n",
    "        for x in 1:60000\n",
    "            temp = Array{UInt8, 1}()\n",
    "            readbytes!(f, temp, 28*28)\n",
    "            push!(train_datas, temp)\n",
    "        end\n",
    "    end\n",
    "    open(filepath*test_label_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 8)\n",
    "        readbytes!(f, test_labels, 10000)\n",
    "    end\n",
    "    open(filepath*test_data_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 16)\n",
    "        for x in 1:10000\n",
    "            temp = Array{UInt8, 1}()\n",
    "            readbytes!(f, temp, 28*28)\n",
    "            push!(test_datas, temp)\n",
    "        end\n",
    "    end\n",
    "    return MNIST(train_datas, train_labels, test_datas, test_labels)\n",
    "end\n",
    "\n",
    "mnist = load_mnist()\n",
    "println(size(mnist.test_datas), size(mnist.train_datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(Any[[0.4413361169379262 0.4833344564494011 ‚Ä¶ 0.5838152403520882 0.7548351085188092; 0.5473357351732486 0.955858461188932 ‚Ä¶ 0.6868857827205286 0.37147868569241504; ‚Ä¶ ; 0.31478076520964327 0.4054403837430205 ‚Ä¶ 0.318559927429068 0.0011768274174845939; 0.0340892763029077 0.41791397359988447 ‚Ä¶ 0.2994023368598915 0.8922630047572884], [0.8008054347810603 0.5120252114070658 ‚Ä¶ 0.6915723254969965 0.9731168736749531; 0.4264208394612492 0.45985464075511406 ‚Ä¶ 0.33061107330112005 0.5116013921459024; ‚Ä¶ ; 0.08807712279080482 0.844810192903988 ‚Ä¶ 0.11782260345904172 0.0683175627627366; 0.46409452296463694 0.7612055332747163 ‚Ä¶ 0.06166663588574761 0.2177929536447527], [0.28352289184533497 0.24801943005848925 ‚Ä¶ 0.8410612646185245 0.2749781640581823; 0.7186764564600143 0.35769651927479185 ‚Ä¶ 0.7809215344190703 0.07711913931912528; ‚Ä¶ ; 0.5117611635749404 0.963042176706915 ‚Ä¶ 0.959441666647699 0.2567896722288636; 0.03602084467714839 0.1584638189983094 ‚Ä¶ 0.558497900403804 0.4250423752285326]], Any[[0.7705600079765598, 0.28258784637044165, 0.5954661011087865, 0.6492148383845475, 0.18072769461772142, 0.8309477582874687, 0.7533709827315307, 0.3922029005271599, 0.24853697925133877, 0.21231369610579365  ‚Ä¶  0.0443113056686526, 0.19835762113746602, 0.32679199755493604, 0.5875697730145886, 0.07114031326946946, 0.39210444307378656, 0.2876754798029615, 0.6207286459222585, 0.9653952463817552, 0.6257948215629705], [0.4972752656138564, 0.9581646558751744, 0.027682964127165866, 0.12277451740676315, 0.38513955741382766, 0.9486209015479425, 0.6727686788659326, 0.06444032842068048, 0.5715378540318334, 0.8433612092711709  ‚Ä¶  0.6171087595961924, 0.4779679373060748, 0.3389820037477371, 0.3764538444003107, 0.7177960353746815, 0.4770553432063016, 0.28157771238969853, 0.8703220764182575, 0.4759962580214192, 0.3008204747092271], [0.2656269339720403, 0.27972306717571493, 0.4741124553285567, 0.40072915550405375, 0.021085838818261537, 0.18593874659244514, 0.13781783613500087, 0.6256597269997051, 0.26262754016723044, 0.5819375997032343]], sigmoid)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(ùíõ) = 1.0/(1.0+exp(-ùíõ))\n",
    "softmax(data) = [exp(x)/sum(exp.(data)) for x in data]\n",
    "\n",
    "struct Network\n",
    "    weights::AbstractArray\n",
    "    biases::AbstractArray\n",
    "    activation::Any\n",
    "end\n",
    "\n",
    "function initnetwork(sizes::Union{Tuple, AbstractArray}, activation::Any)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for (x, y) in [(sizes[x], sizes[x+1]) for x in 1:(length(sizes)-1)]\n",
    "        push!(weights, rand(y, x))\n",
    "        push!(biases, rand(y))\n",
    "    end\n",
    "    return Network(weights, biases, activation)\n",
    "end\n",
    "\n",
    "function loss(net::Network, data::AbstractArray, label)\n",
    "    for x in 1:length(net.weights)\n",
    "        data = net.activation.(net.weights[x] * data + net.biases[x])\n",
    "    end\n",
    "    data = softmax(data)\n",
    "    # notice : this for-loop must start with 0, because the MNIST'data is 0-9\n",
    "    onehot = [x == label ? 1.0 : 0.0 for x in 0:(length(net.biases[end])-1)]\n",
    "    return sum((data - onehot).^2 / length(data))\n",
    "end\n",
    "\n",
    "function batch_loss(net::Network, one_batch_datas::AbstractArray, one_batch_labels::AbstractArray)\n",
    "    total_loss = 0.0\n",
    "    for x in 1:length(one_batch_datas)\n",
    "        total_loss += loss(net, one_batch_datas[x], one_batch_labels[x])\n",
    "    end\n",
    "    return total_loss / length(one_batch_datas)\n",
    "end\n",
    "\n",
    "function shuffle(len, batch_size)\n",
    "    len % batch_size != 0 && error(\"batch_size parameter wrong\")\n",
    "    batches = [x for x in 1:len]\n",
    "    for x in 1:10000\n",
    "        temp = abs.(rand(Int64, 2)).% len\n",
    "        temp = temp.+1\n",
    "        batches[temp[1]], batches[temp[2]] = batches[temp[2]], batches[temp[1]]\n",
    "    end\n",
    "    \n",
    "    return batches\n",
    "end\n",
    "\n",
    "function update(net::Network, one_batch_datas::AbstractArray, one_batch_labels::AbstractArray)\n",
    "    learning_rate = 0.1\n",
    "    loss‚ÇÅ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "    \n",
    "    # calculate gradient for every parameter\n",
    "    weights_dx = deepcopy(net.weights)\n",
    "    biases_dx = deepcopy(net.biases)\n",
    "    \n",
    "    for cind in CartesianIndices(net.weights)\n",
    "        for i in CartesianIndices(net.weights[cind])\n",
    "            net.weights[cind][i] = net.weights[cind][i] + 0.00001\n",
    "            loss‚ÇÇ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "            Œ∑ = (loss‚ÇÇ - loss‚ÇÅ) / 0.00001\n",
    "            weights_dx[cind][i] = -learning_rate * Œ∑\n",
    "            net.weights[cind][i] = net.weights[cind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for bind in CartesianIndices(net.biases)\n",
    "        for i in CartesianIndices(net.biases[bind])\n",
    "            net.biases[bind][i] = net.biases[bind][i] + 0.00001\n",
    "            loss‚ÇÇ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "            Œ∑ = (loss‚ÇÇ - loss‚ÇÅ) / 0.00001\n",
    "            biases_dx[bind][i] = -learning_rate * Œ∑\n",
    "            net.biases[bind][i] = net.biases[bind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for x in length(net.weights)\n",
    "        net.weights[x] = net.weights[x].-weights_dx[x]\n",
    "    end\n",
    "    for x in length(net.biases)\n",
    "        net.biases[x] = net.biases[x].-biases_dx[x]\n",
    "    end\n",
    "end\n",
    "\n",
    "function train(net::Network, datas::AbstractArray, labels::AbstractArray, mini_batches::Int64)\n",
    "    len = length(datas)\n",
    "    for sround_index in 1:20\n",
    "        index_batches = shuffle(len, mini_batches)\n",
    "        rounds = len / mini_batches\n",
    "        for x in 0:(rounds-1)\n",
    "            sind = Int64((x*mini_batches+1))\n",
    "            eind = Int64((x*mini_batches+mini_batches))\n",
    "            one_batch_datas = [datas[index_batches[t]] for t in sind:eind]\n",
    "            one_batch_labels = [datas[index_batches[t]] for t in sind:eind]\n",
    "            update(net, one_batch_datas, one_batch_labels)\n",
    "            println(\"rounds $(sround_index) $(sind)-$(eind) has updated over.\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "network = initnetwork([784, 300, 100, 10], sigmoid)\n",
    "#loss(network, mnist.train_datas[3], mnist.train_labels[3])\n",
    "#train(network, mnist.train_datas, mnist.train_labels, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.67606532906383e-15\n"
     ]
    }
   ],
   "source": [
    "# test for gradient\n",
    "net_test = initnetwork([20, 10, 5, 2], sigmoid)\n",
    "data = rand(20)\n",
    "label = 1\n",
    "zygote_grad = gradient(loss, net_test, data, label)[1]\n",
    "\n",
    "function my_gradient(loss_function, parameters...)\n",
    "    net = parameters[1]\n",
    "    data = parameters[2]\n",
    "    label = parameters[3]\n",
    "    loss‚ÇÅ = loss_function(net, data, label)\n",
    "    \n",
    "    # calculate gradient for every parameter\n",
    "    weights_dx = deepcopy(net.weights)\n",
    "    biases_dx = deepcopy(net.biases)\n",
    "    \n",
    "    for cind in CartesianIndices(net.weights)\n",
    "        for i in CartesianIndices(net.weights[cind])\n",
    "            net.weights[cind][i] = net.weights[cind][i] + 0.00001\n",
    "            loss‚ÇÇ = loss_function(net, data, label)\n",
    "            weights_dx[cind][i] = (loss‚ÇÇ - loss‚ÇÅ) / 0.00001\n",
    "            net.weights[cind][i] = net.weights[cind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for bind in CartesianIndices(net.biases)\n",
    "        for i in CartesianIndices(net.biases[bind])\n",
    "            net.biases[bind][i] = net.biases[bind][i] + 0.00001\n",
    "            loss‚ÇÇ = loss_function(net, data, label)\n",
    "            biases_dx[bind][i] = (loss‚ÇÇ - loss‚ÇÅ) / 0.00001\n",
    "            net.biases[bind][i] = net.biases[bind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    return (weights_dx, biases_dx)\n",
    "end\n",
    "\n",
    "my_grad = my_gradient(loss, net_test, data, label)\n",
    "\n",
    "#println(\"zygote gradients:\")\n",
    "#println(zygote_grad[:weights])\n",
    "#println(\"my gradients:\")\n",
    "#println(my_grad[1])\n",
    "diff = zygote_grad[:weights].-my_grad[1]\n",
    "diff[1] = reshape(diff[1], 200, 1)\n",
    "diff[2] = reshape(diff[2], 50, 1)\n",
    "diff[3] = reshape(diff[3], 10, 1)\n",
    "diff = [diff[1];diff[2];diff[3]].^2\n",
    "println(max(diff...))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
