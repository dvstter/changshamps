{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\bar CÌ„\n",
    "# \\dot ÄŠ\n",
    "# \\ddot CÌˆ\n",
    "using Zygote\n",
    "using Flux: Optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)(60000,)\n"
     ]
    }
   ],
   "source": [
    "struct MNIST\n",
    "    train_datas::AbstractArray\n",
    "    train_labels::AbstractArray\n",
    "    test_datas::AbstractArray\n",
    "    test_labels::AbstractArray\n",
    "end\n",
    "\n",
    "function load_mnist(filepath=\"/Users/yanghanlin/Desktop/julia-2-week/dataset/\")\n",
    "    train_label_file = \"train-labels-idx1-ubyte\"\n",
    "    train_data_file = \"train-images-idx3-ubyte\"\n",
    "    \n",
    "    test_label_file = \"t10k-labels-idx1-ubyte\"\n",
    "    test_data_file = \"t10k-images-idx3-ubyte\"\n",
    "    \n",
    "    train_labels = Array{UInt8, 1}()\n",
    "    train_datas = []\n",
    "    \n",
    "    test_labels = Array{UInt8, 1}()\n",
    "    test_datas = []\n",
    "    \n",
    "    devnull = Array{UInt8, 1}()\n",
    "    open(filepath*train_label_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 8)\n",
    "        readbytes!(f, train_labels, 60000)\n",
    "    end\n",
    "    open(filepath*train_data_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 16)\n",
    "        for x in 1:60000\n",
    "            temp = Array{UInt8, 1}()\n",
    "            readbytes!(f, temp, 28*28)\n",
    "            push!(train_datas, temp)\n",
    "        end\n",
    "    end\n",
    "    open(filepath*test_label_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 8)\n",
    "        readbytes!(f, test_labels, 10000)\n",
    "    end\n",
    "    open(filepath*test_data_file, \"r\") do f\n",
    "        readbytes!(f, devnull, 16)\n",
    "        for x in 1:10000\n",
    "            temp = Array{UInt8, 1}()\n",
    "            readbytes!(f, temp, 28*28)\n",
    "            push!(test_datas, temp)\n",
    "        end\n",
    "    end\n",
    "    return MNIST(train_datas, train_labels, test_datas, test_labels)\n",
    "end\n",
    "\n",
    "mnist = load_mnist()\n",
    "println(size(mnist.test_datas), size(mnist.train_datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(Any[[0.7237922963923853 0.6554228062811784 â€¦ 0.7689862749578893 0.9841248628473738; 0.12985747553274796 0.9497392789374135 â€¦ 0.6441773691199373 0.5317987525792796; â€¦ ; 0.9344136221247197 0.9514445809523902 â€¦ 0.8427660604334992 0.823790077252432; 0.5060860165035468 0.966243961404085 â€¦ 0.3899097690675617 0.29574772408865435], [0.5426633288489173 0.025882430873772178 â€¦ 0.1876816543149895 0.3954284709640379; 0.4573961128859232 0.45901984126079465 â€¦ 0.8199060053623943 0.712877385180714; â€¦ ; 0.18754308551472554 0.2651147120442674 â€¦ 0.5473242036260049 0.7693437753834615; 0.7064610687340045 0.42908694076219667 â€¦ 0.2998091171021007 0.9055932671170113], [0.907470263399416 0.8838042537929625 â€¦ 0.9584943699290756 0.4506238723339284; 0.22372925770999363 0.7616421052017557 â€¦ 0.7947907201148801 0.6546258637141764; â€¦ ; 0.3437928519480855 0.13585848033821257 â€¦ 0.6836439216173058 0.3824079704934342; 0.19086759961778998 0.8346970632853368 â€¦ 0.3049678678487222 0.8985239654299826]], Any[[0.035966722808696394, 0.18050762499442308, 0.10984230928261085, 0.6988427975988367, 0.18911639887608556, 0.9799793593226389, 0.5591965963840297, 0.23830572262060867, 0.9357819598561259, 0.6820278057435873  â€¦  0.6201331209786327, 1.0303684193413432e-5, 0.7757366985678853, 0.502216882957391, 0.5369621430232687, 0.1529757605092108, 0.4087457225384896, 0.8043593341509678, 0.9200427948193464, 0.5452802686961831], [0.06719219910956853, 0.6261950692112883, 0.8371196660284848, 0.09200658608681889, 0.9460111417653017, 0.4080143211794538, 0.8611989673111287, 0.43268275436477377, 0.6949344524877008, 0.03451162725573087  â€¦  0.03122122518031656, 0.26410902794469315, 0.9676064581566257, 0.7617546362630312, 0.8444287579195275, 0.5204040939929928, 0.6713150968093349, 0.24154532932128925, 0.5244186519228096, 0.2059876124860709], [0.5188676481757617, 0.06732817432946492, 0.28185711416349646, 0.34092579312575144, 0.8420780282528497, 0.17101547275718332, 0.056024704734372044, 0.8763708532203327, 0.8504350019907487, 0.37392290074515455]], sigmoid)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(ğ’›) = 1.0/(1.0+exp(-ğ’›))\n",
    "softmax(data) = [exp(x)/sum(exp.(data)) for x in data]\n",
    "\n",
    "struct Network\n",
    "    weights::AbstractArray\n",
    "    biases::AbstractArray\n",
    "    activation::Any\n",
    "end\n",
    "\n",
    "function initnetwork(sizes::Union{Tuple, AbstractArray}, activation::Any)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for (x, y) in [(sizes[x], sizes[x+1]) for x in 1:(length(sizes)-1)]\n",
    "        push!(weights, rand(y, x))\n",
    "        push!(biases, rand(y))\n",
    "    end\n",
    "    return Network(weights, biases, activation)\n",
    "end\n",
    "\n",
    "function loss(net::Network, data::AbstractArray, label)\n",
    "    for x in 1:length(net.weights)\n",
    "        data = net.activation.(net.weights[x] * data + net.biases[x])\n",
    "    end\n",
    "    data = softmax(data)\n",
    "    # notice : this for-loop must start with 0, because the MNIST'data is 0-9\n",
    "    onehot = [x == label ? 1.0 : 0.0 for x in 0:(length(net.biases[end])-1)]\n",
    "    return sum((data - onehot).^2 / length(data))\n",
    "end\n",
    "\n",
    "function batch_loss(net::Network, one_batch_datas::AbstractArray, one_batch_labels::AbstractArray)\n",
    "    total_loss = 0.0\n",
    "    for x in 1:length(one_batch_datas)\n",
    "        total_loss += loss(net, one_batch_datas[x], one_batch_labels[x])\n",
    "    end\n",
    "    return total_loss / length(one_batch_datas)\n",
    "end\n",
    "\n",
    "function shuffle(len, batch_size)\n",
    "    len % batch_size != 0 && error(\"batch_size parameter wrong\")\n",
    "    batches = [x for x in 1:len]\n",
    "    for x in 1:10000\n",
    "        temp = abs.(rand(Int64, 2)).% len\n",
    "        temp = temp.+1\n",
    "        batches[temp[1]], batches[temp[2]] = batches[temp[2]], batches[temp[1]]\n",
    "    end\n",
    "    \n",
    "    return batches\n",
    "end\n",
    "\n",
    "#=\n",
    "# This function will calculate gradient with statical method and then update the network's parameters\n",
    "# This function has been deprecated by yhl because the effiency of this method is very low\n",
    "function update(net::Network, one_batch_datas::AbstractArray, one_batch_labels::AbstractArray)\n",
    "    learning_rate = 0.1\n",
    "    lossâ‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "    \n",
    "    # calculate gradient for every parameter\n",
    "    weights_dx = deepcopy(net.weights)\n",
    "    biases_dx = deepcopy(net.biases)\n",
    "    \n",
    "    for cind in CartesianIndices(net.weights)\n",
    "        for i in CartesianIndices(net.weights[cind])\n",
    "            net.weights[cind][i] = net.weights[cind][i] + 0.00001\n",
    "            lossâ‚‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "            Î· = (lossâ‚‚ - lossâ‚) / 0.00001\n",
    "            weights_dx[cind][i] = -learning_rate * Î·\n",
    "            net.weights[cind][i] = net.weights[cind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for bind in CartesianIndices(net.biases)\n",
    "        for i in CartesianIndices(net.biases[bind])\n",
    "            net.biases[bind][i] = net.biases[bind][i] + 0.00001\n",
    "            lossâ‚‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "            Î· = (lossâ‚‚ - lossâ‚) / 0.00001\n",
    "            biases_dx[bind][i] = -learning_rate * Î·\n",
    "            net.biases[bind][i] = net.biases[bind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for x in length(net.weights)\n",
    "        net.weights[x] = net.weights[x].-weights_dx[x]\n",
    "    end\n",
    "    for x in length(net.biases)\n",
    "        net.biases[x] = net.biases[x].-biases_dx[x]\n",
    "    end\n",
    "end\n",
    "=#\n",
    "\n",
    "function get_gradient(net::Network, one_batch_datas::AbstractArray, one_batch_labels::AbstractArray)\n",
    "    lossâ‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "    \n",
    "    # calculate gradient for every parameter\n",
    "    weights_dx = deepcopy(net.weights)\n",
    "    biases_dx = deepcopy(net.biases)\n",
    "    \n",
    "    zygote_grad = gradient(batch_loss, net, one_batch_dats, one_batch_labels)[1]\n",
    "    weights_dx = zygote_grad[:weights]\n",
    "    biases_dx = zygote_grad[:biases]\n",
    "    \n",
    "    return weights_dx, biases_dx\n",
    "end\n",
    "\n",
    "function train(net::Network, datas::AbstractArray, labels::AbstractArray, mini_batches::Int64)\n",
    "    \n",
    "    len = length(datas)\n",
    "    for sround_index in 1:20\n",
    "        index_batches = shuffle(len, mini_batches)\n",
    "        rounds = len / mini_batches\n",
    "        for x in 0:(rounds-1)\n",
    "            sind = Int64((x*mini_batches+1))\n",
    "            eind = Int64((x*mini_batches+mini_batches))\n",
    "            one_batch_datas = [datas[index_batches[t]] for t in sind:eind]\n",
    "            one_batch_labels = [datas[index_batches[t]] for t in sind:eind]\n",
    "            # deprecated!\n",
    "            #update(net, one_batch_datas, one_batch_labels)\n",
    "            weights_dx, biases_dx = get_gradient(net, one_batch_datas, one_batch_labels)\n",
    "            \n",
    "            \n",
    "            println(\"rounds $(sround_index) $(sind)-$(eind) has updated over.\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "network = initnetwork([784, 300, 100, 10], sigmoid)\n",
    "#loss(network, mnist.train_datas[3], mnist.train_labels[3])\n",
    "#train(network, mnist.train_datas, mnist.train_labels, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.722108 seconds (19.97 M allocations: 1.003 GiB)\n",
      "  0.321412 seconds (1.02 M allocations: 52.738 MiB)\n",
      "3.9553869014512385e-15\n"
     ]
    }
   ],
   "source": [
    "# NOTESï¼šè¿™æ¬¡ç”¨Zygoteä¸æˆ‘è‡ªå·±å†™çš„gradientè¿›è¡Œæ¯”è¾ƒï¼Œçœ‹æœ€å¤§å·®å€¼\n",
    "# é‡è¦çš„æºä»£ç å…¨éƒ¨æ‘˜å½•è‡ªä¸Šé¢æˆ‘å†™çš„ç½‘ç»œ\n",
    "# test for gradient\n",
    "net_test = initnetwork([20, 10, 5, 2], sigmoid)\n",
    "data = rand(20)\n",
    "label = 1\n",
    "@time zygote_grad = gradient(loss, net_test, data, label)[1]\n",
    "\n",
    "function my_gradient(loss_function, parameters...)\n",
    "    net = parameters[1]\n",
    "    data = parameters[2]\n",
    "    label = parameters[3]\n",
    "    lossâ‚ = loss_function(net, data, label)\n",
    "    \n",
    "    # calculate gradient for every parameter\n",
    "    weights_dx = deepcopy(net.weights)\n",
    "    biases_dx = deepcopy(net.biases)\n",
    "    \n",
    "    for cind in CartesianIndices(net.weights)\n",
    "        for i in CartesianIndices(net.weights[cind])\n",
    "            net.weights[cind][i] = net.weights[cind][i] + 0.00001\n",
    "            lossâ‚‚ = loss_function(net, data, label)\n",
    "            weights_dx[cind][i] = (lossâ‚‚ - lossâ‚) / 0.00001\n",
    "            net.weights[cind][i] = net.weights[cind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for bind in CartesianIndices(net.biases)\n",
    "        for i in CartesianIndices(net.biases[bind])\n",
    "            net.biases[bind][i] = net.biases[bind][i] + 0.00001\n",
    "            lossâ‚‚ = loss_function(net, data, label)\n",
    "            biases_dx[bind][i] = (lossâ‚‚ - lossâ‚) / 0.00001\n",
    "            net.biases[bind][i] = net.biases[bind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    return (weights_dx, biases_dx)\n",
    "end\n",
    "\n",
    "@time my_grad = my_gradient(loss, net_test, data, label)\n",
    "\n",
    "#println(\"zygote gradients:\")\n",
    "#println(zygote_grad[:weights])\n",
    "#println(\"my gradients:\")\n",
    "#println(my_grad[1])\n",
    "diff = zygote_grad[:weights].-my_grad[1]\n",
    "diff[1] = reshape(diff[1], 200, 1)\n",
    "diff[2] = reshape(diff[2], 50, 1)\n",
    "diff[3] = reshape(diff[3], 10, 1)\n",
    "diff = [diff[1];diff[2];diff[3]].^2\n",
    "println(max(diff...))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.434560 seconds (898.25 k allocations: 44.707 MiB)\n",
      "  0.025957 seconds (48.37 k allocations: 2.476 MiB)\n",
      "[9.409162501187021e-14; 3.747953336995626e-16; 2.3806047904785495e-13; 3.7342498520762885e-15; 7.61357723984382e-15; 1.7039840103914804e-16; 2.0280137833107334e-15; 1.2627206898808941e-13; 5.082964651534606e-14; 5.6254938208633995e-15; 3.7912161245325983e-13; 5.1397946423112165e-14; 4.485257194913899e-13; 1.1102333569614331e-13; 8.2345435182776e-14; 1.3986377428013403e-12; 2.663607081520241e-13; 1.401569445074848e-13; 7.520117762991672e-14; 1.1673291015354743e-14; 1.5298395969613618e-12; 4.8187709742441606e-14; 3.0074062483607323e-12; 1.6397841699246046e-13; 1.9010434220675373e-13; 8.528891757822658e-13; 2.5218367020621e-13; 1.386842646099919e-12; 6.003082767340809e-13; 7.311773235812413e-14; 1.0118109904807181e-12; 6.794335272311561e-14; 1.6254683719353314e-12; 1.7755444127306509e-13; 1.627964557016037e-13; 1.5792250944957715e-12; 3.534558081943825e-13; 6.553418367101549e-13; 3.0485403894956664e-13; 4.053000786758357e-14; 1.3321249445409705e-12; 1.2668907863091572e-13; 1.874688911506478e-12; 2.9943812474556756e-13; 2.4680813667732584e-13; 3.210828537989179e-12; 6.577051543749004e-13; 6.828035881966414e-13; 3.359087850853912e-13; 4.757976690618671e-14; 1.5007823656977332e-14; 2.1712528876962964e-15; 1.7109323082592615e-14; 4.6187641179918924e-15; 3.3619010895083175e-15; 5.975964834280752e-14; 1.1248538405559813e-14; 5.156452493507691e-15; 2.825828178651577e-15; 4.477442040598278e-16; 1.3389114562051526e-14; 2.9678740029433322e-15; 1.1297499793551551e-14; 5.7651398550326e-15; 3.720550377465569e-15; 8.720869462352223e-14; 1.5348290935880594e-14; 2.3946322743266212e-15; 1.6218842561885139e-15; 3.104721873085781e-16; 3.0750242580330964e-13; 1.0894452890733752e-15; 7.837701677937319e-13; 1.171682602575821e-14; 2.4506599994410382e-14; 1.3065162100130464e-16; 5.9178896457283526e-15; 4.172853182465448e-13; 1.6764820610327152e-13; 1.8504586736574808e-14; 4.73720150801665e-13; 5.86132577993028e-14; 5.887299501005879e-13; 1.2943750052254896e-13; 9.861265500205728e-14; 1.5682466030851405e-12; 3.038820379031088e-13; 1.9264239912566257e-13; 1.0066738667839977e-13; 1.5209601662686896e-14; 7.316965570025321e-13; 1.7293124686281634e-14; 1.519796889726211e-12; 6.619848605110544e-14; 8.378066140398996e-14; 2.6387026754541224e-13; 9.076880131574135e-14; 7.242973653395608e-13; 3.081371646650995e-13; 3.6710038667998436e-14; 1.4185977358780995e-12; 6.4904379647318916e-15; 3.5554959445823366e-12; 5.920984110100895e-14; 1.1695742954539159e-13; 7.0626144783210095e-15; 3.50193199854933e-14; 1.876975349188063e-12; 7.573700992791399e-13; 8.409172638297333e-14; 2.444302267899208e-13; 1.3358253646250538e-14; 4.190105889497059e-13; 3.727424120126546e-14; 3.6427785885047905e-14; 2.921339073145218e-13; 6.959259034076373e-14; 1.7683118793457173e-13; 8.025308141029987e-14; 1.0360716047085028e-14; 2.508935553400402e-12; 6.358572866713648e-14; 5.14647816107093e-12; 2.363197866135118e-13; 2.9282502695786803e-13; 1.0088578190391581e-12; 3.3347645366754973e-13; 2.4347167965989715e-12; 1.0397983245302023e-12; 1.2449613595323296e-13; 5.699245562927681e-13; 5.6162468333238294e-15; 1.3358756695737221e-12; 3.2759323147455584e-14; 5.335299133363192e-14; 4.01077821088404e-14; 2.983491222964438e-14; 6.811218174937979e-13; 2.7977992012081947e-13; 3.18261773707605e-14; 6.266228682742945e-13; 4.1555304287520755e-14; 1.0109087148037754e-12; 1.0901368591253296e-13; 1.0033536998112341e-13; 9.624928585025254e-13; 2.1619141475270515e-13; 4.088003226825377e-13; 1.898547685237367e-13; 2.5193094408470182e-14; 6.562148005858155e-13; 1.15384800553582e-13; 6.606851489469174e-13; 2.350273785373529e-13; 1.6175081519725186e-13; 3.279238991596874e-12; 5.972983777304857e-13; 1.7456166065717177e-13; 1.0334043494832607e-13; 1.7626446364254196e-14; 2.974873574209323e-13; 6.852621918568235e-14; 2.428224012580455e-13; 1.321119174666877e-13; 8.437889633410551e-14; 2.0243993043086676e-12; 3.543784967851057e-13; 4.913417524124518e-14; 3.4245216351902926e-14; 6.709320487649656e-15; 8.968671290833409e-13; 2.4740098884978503e-14; 1.8106517894044468e-12; 8.878131360786064e-14; 1.0720775254027643e-13; 4.1051530606488206e-13; 1.2964346436757578e-13; 8.484988787765863e-13; 3.6418853506886943e-13; 4.3884741945380214e-14; 3.166421201061386e-12; 1.3374834589363696e-13; 5.820948828935085e-12; 4.0769243872395376e-13; 4.316300796860673e-13; 2.6759329334962814e-12; 6.981357860823731e-13; 2.5734289388036173e-12; 1.139126003119629e-12; 1.42678611873496e-13; 1.18587898411438e-12; 6.584044308663042e-14; 2.023370876505807e-12; 1.8275644738583832e-13; 1.7772355868239876e-13; 1.4469784234169656e-12; 3.429566229250828e-13; 8.511483981763027e-13; 3.8694064015749915e-13; 5.005980426096278e-14; 8.240381391395991e-10; 1.494069813705273e-9; 2.4589760552201355e-8; 1.51725954568771e-10; 1.7069159533511385e-9; 8.36327616573059e-10; 1.5216803096418825e-9; 2.4687675492201907e-8; 1.5427013838495297e-10; 1.7402930076794287e-9; 8.30403122269873e-10; 1.5122326178291242e-9; 2.4445885215196938e-8; 1.5324765484665587e-10; 1.729945924783084e-9; 8.316767330614755e-10; 1.5056126037532983e-9; 2.493465209040401e-8; 1.5301057179531668e-10; 1.719311124017583e-9; 8.302438756586781e-10; 1.504210870142629e-9; 2.4831183567367376e-8; 1.5280989066231024e-10; 1.718119790787848e-9; 7.822368634472524e-10; 1.3974108849360268e-9; 2.441467585967253e-8; 1.4292541347066285e-10; 1.5893509879473258e-9; 8.277674333095917e-10; 1.493621411726026e-9; 2.5067738241062697e-8; 1.5203166514760618e-10; 1.7039306449151377e-9; 8.308886180371289e-10; 1.5163085192369202e-9; 2.430049802704476e-8; 1.5350565669988925e-10; 1.7357025725098965e-9; 8.365977523761947e-10; 1.5295967440468103e-9; 2.4324390134705448e-8; 1.547114646409095e-10; 1.7518977877956613e-9; 8.416729646467011e-10; 1.5295226885230826e-9; 2.4940293322492652e-8; 1.5515669058090426e-10; 1.748615232217067e-9; 9.807776368370635e-5; 0.0021548364454739783; 9.870515063846725e-5; 0.002123946666821535; 0.00010365719919728936; 0.002105433769299413; 0.00010194392447239636; 0.002148419422079666; 0.00010228953357956743; 0.0021436005241537567]\n"
     ]
    }
   ],
   "source": [
    "# NOTESï¼šè¿™æ¬¡ç”¨Zygoteä¸æˆ‘è‡ªå·±å†™çš„batch_gradientè¿›è¡Œæ¯”è¾ƒï¼Œçœ‹æœ€å¤§å·®å€¼\n",
    "# é‡è¦çš„æºä»£ç å…¨éƒ¨æ‘˜å½•è‡ªä¸Šé¢æˆ‘å†™çš„ç½‘ç»œ\n",
    "# test for batch gradient\n",
    "net_test2 = initnetwork([20, 10, 5, 2], sigmoid)\n",
    "one_batch_datas = [rand(20) for x in 100]\n",
    "one_batch_labels = [abs(rand(Int64))%2 for x in 100]\n",
    "@time zygote_grad = gradient(batch_loss, net_test2, one_batch_datas, one_batch_labels)[1]\n",
    "\n",
    "function my_batch_gradient(net::Network, one_batch_datas::AbstractArray, one_batch_labels::AbstractArray)\n",
    "    lossâ‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "    \n",
    "    # calculate gradient for every parameter\n",
    "    weights_dx = deepcopy(net.weights)\n",
    "    biases_dx = deepcopy(net.biases)\n",
    "    \n",
    "    for cind in CartesianIndices(net.weights)\n",
    "        for i in CartesianIndices(net.weights[cind])\n",
    "            net.weights[cind][i] = net.weights[cind][i] + 0.00001\n",
    "            lossâ‚‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "            weights_dx[cind][i] = (lossâ‚‚ - lossâ‚) / 0.00001\n",
    "            net.weights[cind][i] = net.weights[cind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for bind in CartesianIndices(net.biases)\n",
    "        for i in CartesianIndices(net.biases[bind])\n",
    "            net.biases[bind][i] = net.biases[bind][i] + 0.00001\n",
    "            lossâ‚‚ = batch_loss(net, one_batch_datas, one_batch_labels)\n",
    "            biases_dx[bind][i] = (lossâ‚‚ - lossâ‚) / 0.00001\n",
    "            net.biases[bind][i] = net.biases[bind][i] - 0.00001\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (weights_dx, biases_dx)\n",
    "end\n",
    "\n",
    "@time my_batch_grad = my_batch_gradient(net_test, one_batch_datas, one_batch_labels)\n",
    "\n",
    "#println(\"zygote gradients:\")\n",
    "#println(zygote_grad[:weights])\n",
    "#println(\"my gradients:\")\n",
    "#println(my_grad[1])\n",
    "diff = zygote_grad[:weights].-my_grad[1]\n",
    "diff[1] = reshape(diff[1], 200, 1)\n",
    "diff[2] = reshape(diff[2], 50, 1)\n",
    "diff[3] = reshape(diff[3], 10, 1)\n",
    "diff = [diff[1];diff[2];diff[3]].^2\n",
    "println(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
